## Upload training data to Gcloud storage ##
gsutil cp -r local/path/data.pickle gs://data/data.pickle

## Connect server instance to SSH (in gcloud shell)##
gcloud compute --project "qbot-209820" ssh --zone "us-central1-c" "instance-1"

## Now commads can be executed in server instance though gcloud shell ##




## Commands set to run job ##
export JOB_NAME="test_job" #update with jon label for each run
export BUCKET_NAME=aleksey-konoiko-storage 
export CLOUD_CONFIG=trainer/cloudml-gpu.yaml
export JOB_DIR=gs://aleksey-konoiko-storage/jobs/$JOB_NAME
export MODULE=trainer.cloud_trainer
export PACKAGE_PATH=./trainer
export REGION=us-central1
export RUNTIME=1.2
export TRAIN_FILE=gs://data/input/data.pickle

gcloud ml-engine jobs submit training $JOB_NAME \
    --job-dir $JOB_DIR \
    --runtime-version $RUNTIME \
    --module-name $MODULE \
    --package-path $PACKAGE_PATH \
    --region $REGION \
    --config=$CLOUD_CONFIG \
    -- \
    --train-file $TRAIN_FILE \
    --job-name $JOB_NAME


## Run tensorboard when job submitted (run cloud shell)##
tensorboard --port 8080 --logdir gs://aleksey-konoiko-storage/jobs/test_job/logs
