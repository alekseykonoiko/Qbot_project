## Upload training data to Gcloud storage ##
gsutil cp -r local/path/data.pickle gs://data/data.pickle

## From local to instance ##
gcloud compute scp [LOCAL_FILE_PATH] [INSTANCE_NAME]:~/
## From instance to local ##
gcloud compute scp --recurse [INSTANCE_NAME]:[REMOTE_DIR] [LOCAL_DIR]

## Connect server instance to SSH (in gcloud shell)##
gcloud compute --project "qbot-209820" ssh --zone "us-central1-c" "instance-1"

## Now commads can be executed in server instance though gcloud shell ##

## Install essential Ubuntu packages ##
sudo apt-get update

sudo apt-get install \
     apt-transport-https \
     ca-certificates \
     curl \
     gnupg2 \
     software-properties-common

## Install Docker ##
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add - #add docker public key
sudo apt-key fingerprint 0EBFCD88 #verify that key added
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get update
apt-cache policy docker-ce
sudo apt-get install -y docker-ce
sudo docker run hello-world #verify docker installation

## When installing specific docker version ##
apt-cache madison docker-ce #lists available docker versions in the repo

sudo apt-get install docker-ce=<VERSION_STRING> #for example docker-ce=18.03.0.ce

sudo docker run hello-world #verify docker installation

## Install nvidia-docker2 ##
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \
  sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update

sudo apt-get install nvidia-docker2

sudo pkill -SIGHUP dockerd

## Test nvidia-docker2 ##

sudo docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi




## Run tensorboard when job submitted (run cloud shell)##
tensorboard --port 8080 --logdir gs://aleksey-konoiko-storage/jobs/test_job/logs












## Commands set to run job ##
export JOB_NAME="test_job" #update with jon label for each run
export BUCKET_NAME=aleksey-konoiko-storage 
export CLOUD_CONFIG=trainer/cloudml-gpu.yaml
export JOB_DIR=gs://aleksey-konoiko-storage/jobs/$JOB_NAME
export MODULE=trainer.cloud_trainer
export PACKAGE_PATH=./trainer
export REGION=us-central1
export RUNTIME=1.2
export TRAIN_FILE=gs://data/input/data.pickle

gcloud ml-engine jobs submit training $JOB_NAME \
    --job-dir $JOB_DIR \
    --runtime-version $RUNTIME \
    --module-name $MODULE \
    --package-path $PACKAGE_PATH \
    --region $REGION \
    --config=$CLOUD_CONFIG \
    -- \
    --train-file $TRAIN_FILE \
    --job-name $JOB_NAME



